{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "<a href=\"https://archive.ics.uci.edu/ml/datasets/online+retail\">Online retail</a> is a transnational dataset which contains all the transactions occurring between 01/12/2010 and 09/12/2011 for a UK-based and registered non-store online retail. The company mainly sells unique all-occasion gifts. Many customers of the company are wholesalers.\n",
    "\n",
    "## Source\n",
    "UCI Machine Learning Repository: https://archive.ics.uci.edu/ml/datasets/online+retail\n",
    "\n",
    "## Business Goal\n",
    "To segment the Customers based on RFM so that the company can target its customers efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-01T10:16:54.581653Z",
     "start_time": "2020-11-01T10:16:54.570637Z"
    }
   },
   "source": [
    "## Methodology\n",
    "\n",
    "1. [Reading and Understanding the Data](#1) <br>\n",
    "    a. Creating a Data Dictionary\n",
    "2. [Data Cleaning](#2)\n",
    "3. [Data Preparation](#3) <br>\n",
    "    a. Scaling Variables\n",
    "4. [Model Building](#4) <br>\n",
    "    a. K-means Clustering <br>\n",
    "    b. Finding the Optimal K\n",
    "5. [Final Analysis](#5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a> <br>\n",
    "### 1 : Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-01T13:14:00.815005Z",
     "start_time": "2020-11-01T13:13:56.682079Z"
    }
   },
   "outputs": [],
   "source": [
    "# import required libraries for dataframe and visualization\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime as dt\n",
    "import plotly as py \n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import init_notebook_mode, plot, iplot\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "# import required libraries for clustering\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from scipy.cluster.hierarchy import cut_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-01T13:14:02.085599Z",
     "start_time": "2020-11-01T13:14:00.818009Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reading the data on which analysis needs to be done\n",
    "retail = pd.read_csv('dataset/OnlineRetail.csv', encoding='ISO-8859-1')\n",
    "# Display first 10 rows\n",
    "retail.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Dictionary \n",
    "\n",
    "First Header  | Definition    |  Description  | Data Type\n",
    "------------- | ------------- | ------------- | -------------\n",
    "InvoiceNo  | Invoice number | A 6-digit integral number uniquely assigned to each transaction. If this code starts with letter 'c', it indicates a cancellation. | Nominal\n",
    "StockCode | Product (item) code | A 5-digit integral number uniquely assigned to each distinct product. | Nominal\n",
    "Description | Product (item) name | Name of Product | Nominal\n",
    "Quantity | Quantity | The quantities of each product (item) per transaction | Numeric\n",
    "InvoiceDate | Invoice Date and time | The day and time when each transaction was generated. | Numeric\n",
    "UnitPrice | Unit price | Product price per unit in sterling. | Numeric\n",
    "CustomerID | Customer number | A 5-digit integral number uniquely assigned to each customer. | Nominal\n",
    "Country | Country name | The name of the country where each customer resides. | Nominal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = retail.isnull().sum()\n",
    "\n",
    "# Filter columns with missing values (optional, can remove if you want all columns)\n",
    "missing_values = missing_values[missing_values > 0]\n",
    "\n",
    "# Plot the missing values\n",
    "plt.figure(figsize=(10, 6))\n",
    "missing_values.plot(kind='bar', color='skyblue')\n",
    "\n",
    "# Add labels and title\n",
    "plt.title('Missing Values per Column', fontsize=16)\n",
    "plt.xlabel('Columns', fontsize=12)\n",
    "plt.ylabel('Number of Missing Values', fontsize=12)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the majority of the missing values is in the Customer ID. Since Customer ID is a sensitive column, and we cannot make justifications o assumptions of the number of customers coming in to buy as it may cause some data inconsistencies and be the cause of misrepresentation of sensitive sales data, we drop the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the null customer id rows\n",
    "retail_cleaned = retail.copy()\n",
    "retail_cleaned = retail_cleaned.dropna(subset=['CustomerID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try to derive the meaning of the descriptions of the items using Stock Code, and try to fill in missing values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_code_to_description = retail_cleaned.dropna(subset=['Description']).set_index('StockCode')['Description'].to_dict()\n",
    "\n",
    "# Fill missing descriptions using the dictionary\n",
    "retail_cleaned['Description'] = retail_cleaned.apply(\n",
    "    lambda row: stock_code_to_description.get(row['StockCode']) if pd.isnull(row['Description']) else row['Description'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Drop rows where the description is still missing\n",
    "df_cleaned = retail_cleaned.dropna(subset=['Description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if missing values are no more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail_cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF Description\n",
    "retail.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a> <br>\n",
    "### 2 : Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-01T13:14:02.686138Z",
     "start_time": "2020-11-01T13:14:02.492125Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculating the Missing Values % contribution in DF\n",
    "df_null = round(100*(retail.isnull().sum())/len(retail), 2)\n",
    "df_null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-01T13:14:02.884178Z",
     "start_time": "2020-11-01T13:14:02.691137Z"
    }
   },
   "outputs": [],
   "source": [
    "# Droping rows having missing values\n",
    "retail = retail.dropna()\n",
    "retail.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-01T13:14:03.209183Z",
     "start_time": "2020-11-01T13:14:02.890175Z"
    }
   },
   "outputs": [],
   "source": [
    "# Changing the datatype of Customer Id as per Business understanding\n",
    "retail['CustomerID'] = retail['CustomerID'].astype(str)\n",
    "retail.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a> <br>\n",
    "### 3 : Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Customers will be analyzed based on 3 factors:\n",
    "- R (Recency): Number of days since last purchase\n",
    "- F (Frequency): Number of tracsactions\n",
    "- M (Monetary): Total amount of transactions (revenue contributed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail_cleaned['InvoiceDate'] = pd.to_datetime(retail_cleaned['InvoiceDate'], format=\"%d-%m-%Y %H:%M\", errors='coerce')\n",
    "\n",
    "# Step 2: Compute the maximum date to determine the last transaction date\n",
    "last_transaction_date = retail_cleaned['InvoiceDate'].max()\n",
    "print(f\"Last transaction date: {last_transaction_date}\")\n",
    "\n",
    "# Step 3: Compute Recency, Frequency, and Monetary for each customer\n",
    "rfm = retail_cleaned.groupby('CustomerID').agg({\n",
    "    'InvoiceDate': lambda x: (last_transaction_date - x.max()).days,  # Recency: Days since last purchase\n",
    "    'InvoiceNo': 'nunique',                                           # Frequency: Number of unique transactions\n",
    "    'UnitPrice': lambda x: np.sum(x * retail_cleaned.loc[x.index, 'Quantity'])    # Monetary: Total spending per customer\n",
    "}).reset_index()\n",
    "\n",
    "# Step 4: Rename the columns to represent RFM\n",
    "rfm.columns = ['CustomerID', 'Recency', 'Frequency', 'Monetary']\n",
    "\n",
    "# Step 5: Display the resulting RFM table\n",
    "print(rfm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the two dfs\n",
    "retail_final = pd.merge(retail_cleaned, rfm, on='CustomerID', how='inner')  # You can change 'how' to 'outer', 'left', or 'right' as needed\n",
    "\n",
    "# Display the first few rows of the merged DataFrame\n",
    "print(retail_final.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram #1: Number of Transactions per Customer\n",
    "\n",
    "transactions_per_customer = retail.groupby('CustomerID').size()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(transactions_per_customer, bins=50, edgecolor='black')\n",
    "\n",
    "plt.title('Number of Transactions per Customer', fontsize=16)\n",
    "plt.xlabel('Number of Transactions', fontsize=14)\n",
    "plt.ylabel('Number of Customers', fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of the histogram reveals a right-skewed distribution. This means that majority of the customers have only made few transactions, in this point-of-view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram # 2\n",
    "item_counts = retail['Description'].value_counts().sort_values(ascending=False).iloc[0:15]\n",
    "plt.figure(figsize=(18,6))\n",
    "sns.barplot(x=item_counts.index, y=item_counts.values, palette=sns.cubehelix_palette(15))\n",
    "plt.ylabel(\"Counts\")\n",
    "plt.title(\"Which items were bought more often?\")\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram above shows the top 15 items sold. Other than the quite steep frop from the top 3 to top 4, there's no clear pattern or even a reason why there items are popular. There are also no correlation among them as to why they are bought this frequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail['TotalSales'] = retail['Quantity'] * retail['UnitPrice']\n",
    "\n",
    "# Group by StockCode and calculate the total sales per product\n",
    "sales_by_stockcode = retail.groupby('StockCode')['TotalSales'].sum()\n",
    "\n",
    "# Filter out non-positive sales\n",
    "sales_by_stockcode = sales_by_stockcode[sales_by_stockcode > 0]\n",
    "\n",
    "# Create the boxplot\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.boxplot(sales_by_stockcode, vert=False, patch_artist=True, boxprops=dict(facecolor='skyblue', color='black'))\n",
    "plt.title('Boxplot of Total Sales Value by StockCode', fontsize=16)\n",
    "plt.xlabel('Total Sales Value', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The boxplot of \"Total Sales Value\" reveals a right-skewed distribution. This means there are a few outliers that pull the mean to the right, while the majority of sales values are concentrated on the lower end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap\n",
    "\n",
    "# Select relevant columns and drop any rows with NaN values\n",
    "data = retail_final[['Quantity', 'UnitPrice', 'Recency', 'Frequency', 'Monetary']]\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "correlation_matrix = data.corr()\n",
    "\n",
    "# Set the size of the heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Create the heatmap using Seaborn\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, square=True, linewidths=0.5)\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title('Correlation Heatmap between Quantity and UnitPrice', fontsize=16)\n",
    "plt.xticks(rotation=45, fontsize=12)\n",
    "plt.yticks(rotation=45, fontsize=12)\n",
    "\n",
    "# Show the heatmap\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our heatmap, Frequency and Monetary or Amount Spent had the highest strong positive correlation of 0.59, which mean customers who visit frequent likely spend more money. Quantity and Monetary have a correlation of 0.036, which means that customers who buy more in terms of quanitity also spend more. Recency and Frequency surprisingly had -0.24, which means that customers do not have a noticeable pattern on when they visit the store."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rescaling the Attributes\n",
    "\n",
    "It is extremely important to rescale the variables so that they have a comparable scale.<br>\n",
    "There are two common ways of rescaling:\n",
    "\n",
    "1. Min-Max scaling \n",
    "2. Standardization (mean-0, sigma-1) \n",
    "\n",
    "Here we execute Standard Scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescaling the attributes\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "features = ['Monetary', 'Frequency', 'Recency']  \n",
    "X = retail_final[features]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: red;\">Execute MinMax Scaling in the next box</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled = pd.DataFrame(X_scaled)\n",
    "X_scaled.columns = features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot before scaling\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Scatter plot for original data\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(retail_final['Frequency'], retail_final['Monetary'], alpha=0.5)\n",
    "plt.title('Original RFM Data')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Amount')\n",
    "plt.grid(True)\n",
    "\n",
    "# Scatter plot for scaled data\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_scaled['Frequency'], X_scaled['Monetary'], alpha=0.5, color='orange')\n",
    "plt.title('Scaled RFM Data')\n",
    "plt.xlabel('Frequency (Scaled)')\n",
    "plt.ylabel('Amount (Scaled)')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We scale this using MinMaxScaling to reduce the impact of outliers when we try to implement K-Means Clustering later on. Standardized values allow for comparisons across different features more easily. In terms of K-Means, normalization ensures that all features contribute equally to the distance calculations. This is particularly important in algorithms that use Euclidean distances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a> <br>\n",
    "### 4 : Building the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means clustering is one of the simplest and popular unsupervised machine learning algorithms.<br>\n",
    "\n",
    "The algorithm works as follows:\n",
    "\n",
    "- First we initialize k points, called means, randomly.\n",
    "- We categorize each item to its closest mean and we update the mean’s coordinates, which are the averages of the items categorized in that mean so far.\n",
    "- We repeat the process for a given number of iterations and at the end, we have our clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-01T13:14:05.629395Z",
     "start_time": "2020-11-01T13:14:05.508388Z"
    }
   },
   "outputs": [],
   "source": [
    "# k-means with some arbitrary k\n",
    "Kmeans = KMeans(n_clusters=4, max_iter=50)\n",
    "Kmeans.fit(X_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: red;\">Finding the Optimal Number of Clusters</span> \n",
    "\n",
    "#### Elbow Curve to get the right number of Clusters\n",
    "A fundamental step for any unsupervised algorithm is to determine the optimal number of clusters into which the data may be clustered. The Elbow Method is one of the most popular methods to determine this optimal value of k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wcss = []\n",
    "\n",
    "for i in range (1,8):\n",
    "    kmeans = KMeans(n_clusters=i, init = 'k-means++', max_iter=300, n_init=7, random_state=0)\n",
    "    kmeans.fit(X_scaled)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "plt.plot(range(1,8), wcss)\n",
    "plt.title('The elbow method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 is the optimal number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a K_means function here\n",
    "def K_Mean(X, n):\n",
    "    scaler = MinMaxScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    model = KMeans(n)\n",
    "    model.fit(X)\n",
    "    clusters = model.predict(X)\n",
    "    cent = model.cluster_centers_\n",
    "    return (clusters, cent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters, cent = K_Mean(X_scaled, 3)\n",
    "kmeans = pd.DataFrame(clusters)\n",
    "retail_final.insert((retail_final.shape[1]), 'kmeans', kmeans)\n",
    "retail_final.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot your clusters\n",
    "def Plot3dClustering(n, X, type_c):\n",
    "    data = []\n",
    "    clusters = []\n",
    "    colors = ['rgb(228,26,28)', 'rgb(55,126,184)', 'rgb(77,175,74)']\n",
    "    \n",
    "    for i in range(n):\n",
    "        name = i\n",
    "        color = colors[i]\n",
    "        x = X[X[type_c] == i]['Monetary']\n",
    "        y = X[X[type_c] == i]['Frequency']\n",
    "        z = X[X[type_c] == i]['Recency']\n",
    "        \n",
    "        trace = dict(\n",
    "            name = name,\n",
    "            x = x, y = y, z = z,\n",
    "            type = 'scatter3d',\n",
    "            mode = 'markers',\n",
    "            marker = dict(size = 3, color = color, line = dict(width = 0)))\n",
    "        data.append(trace)\n",
    "        \n",
    "        cluster = dict(\n",
    "            color = color,\n",
    "            opacity = 0.1,\n",
    "            type = 'mesh3d',\n",
    "            alphahull = 7,\n",
    "            name = \"y\",\n",
    "            x = x, y = y, z = z)\n",
    "        data.append(cluster)\n",
    "        \n",
    "    layout = dict(\n",
    "        width = 800,\n",
    "        height = 550,\n",
    "        autosize = False,\n",
    "        title = '3D Clustering Plot',\n",
    "        scene = dict(\n",
    "            xaxis = dict(\n",
    "                gridcolor = 'rgb(255, 255, 255)',\n",
    "                zerolinecolor = 'rgb(255, 255, 255)',\n",
    "                showbackground = True,\n",
    "                title='Amount',\n",
    "                backgroundcolor = 'rgb(230, 230, 230)',\n",
    "                ),\n",
    "            yaxis = dict(\n",
    "                gridcolor = 'rgb(255, 255, 255)',\n",
    "                zerolinecolor = 'rgb(255, 255, 255)',\n",
    "                showbackground = True,\n",
    "                title='Frequency',\n",
    "                backgroundcolor = 'rgb(230, 230, 230)',\n",
    "                ),\n",
    "            zaxis = dict(\n",
    "                gridcolor = 'rgb(255, 255, 255)',\n",
    "                zerolinecolor = 'rgb(255, 255, 255)',\n",
    "                showbackground = True,\n",
    "                title='Recency',\n",
    "                backgroundcolor = 'rgb(230, 230, 230)',\n",
    "                ),\n",
    "            aspectratio = dict(x=1, y=1, z=0.7),\n",
    "            aspectmode = 'manual'\n",
    "        ),\n",
    "    )\n",
    "        \n",
    "    fig = dict(data = data, layout = layout)\n",
    "    iplot(fig, filename='3d-scatter-colorscale', validate=False)\n",
    "\n",
    "Plot3dClustering(n=3, X=retail_final, type_c='kmeans')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Silhouette Score and Cluster Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a> <br>\n",
    "## Step 5 : Final Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: red;\">Findings</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Student Name:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
